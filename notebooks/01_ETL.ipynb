{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a5d104",
   "metadata": {},
   "source": [
    "# Cosmos de Cravos\n",
    "\n",
    "## ETL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89db13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "289305ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee190372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import re, os, sys, json\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "\n",
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "# from app.utils.embeddings import OpenAIEmbedding\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd56d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3913ae13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b8d8cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/NOVA/_PhD/arquivo25/cosmos_cravos\n"
     ]
    }
   ],
   "source": [
    "# Get the parent directory\n",
    "parent_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "print(parent_directory)\n",
    "# Add the parent directory to the Python path if it is not already included\n",
    "if parent_directory not in sys.path:\n",
    "    sys.path.append(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9be318ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = os.path.abspath(os.path.join(\"./../../\",\".env\" ))\n",
    "\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "client_gpt = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "CHROMADB_PATH = os.environ.get(\"CHROMADB_PATH\")\n",
    "EMBEDDING_MODEL = os.environ.get(\"DEFAULT_EMBEDDING_MODEL\")\n",
    "COMPLETION_MODEL = os.environ.get(\"DEFAULT_COMPLETION_MODEL\")\n",
    "CHUNK_SIZE = int(os.environ.get(\"DEFAULT_CHUNK_SIZE\"))\n",
    "CHUNK_OVERLAP = int(os.environ.get(\"DEFAULT_CHUNK_OVERLAP\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8d64137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from app.etl.preprocess import JsonDataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0e873ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = os.path.join(\"./../data\", \"res\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeac4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98a6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402f615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f249dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_arq_meta(arq_item, meta_id=0, arq_keys=[]):\n",
    "    \n",
    "    if len(arq_keys) == 0:\n",
    "        arq_keys = ['tstamp', 'title', 'originalURL', 'linkToArchive', 'linkToNoFrame', 'linkToScreenshot']\n",
    "\n",
    "    arq_meta = {k: arq_item[k] for k in arq_keys}\n",
    "    \n",
    "    meta_id += 1\n",
    "    # arq_meta['meta_id'] = meta_id\n",
    "\n",
    "    return arq_meta, meta_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83d064a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by removing HTML tags, normalizing line breaks,\n",
    "    and cleaning up whitespace.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw text from the JSON file\n",
    "\n",
    "    Returns:\n",
    "        str: Preprocessed text\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Remove HTML tags if present\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "    text = text.replace('{html}',\"\") \n",
    "\n",
    "    # Normalize line breaks\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # Remove repeated empty lines (more than 2 consecutive newlines)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    # Strip leading/trailing whitespace from each line\n",
    "    lines = [line.strip() for line in text.split(\"\\n\")]\n",
    "\n",
    "    # Remove empty lines at the beginning and end\n",
    "    while lines and not lines[0]:\n",
    "        lines.pop(0)\n",
    "    while lines and not lines[-1]:\n",
    "        lines.pop()\n",
    "\n",
    "    # Rejoin the lines\n",
    "    processed_text = \"\\n\".join(lines)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc43cfe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a3a71f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def chunk_text(text, encoding=encoding, max_tokens= 8000):\n",
    "    \"\"\"\n",
    "    Splits a long text into sentence-based chunks that stay under OpenAI's max token limit\n",
    "    for text embeddings (8191 for text-embedding-3 models).\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk.\n",
    "        max_tokens (int): Max token limit per chunk (default is 8191).\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (text, token_count)\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_token_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_token_count = len(encoding.encode(sentence))\n",
    "\n",
    "        # If adding sentence doesn't exceed limit, add it\n",
    "        if current_token_count + sentence_token_count <= max_tokens:\n",
    "            if current_chunk:\n",
    "                current_chunk += \" \" + sentence\n",
    "            else:\n",
    "                current_chunk = sentence\n",
    "            current_token_count += sentence_token_count\n",
    "        else:\n",
    "            # Save current chunk\n",
    "            if current_chunk:\n",
    "                chunks.append((current_chunk, current_token_count))\n",
    "            \n",
    "            # Start new chunk\n",
    "            current_chunk = sentence\n",
    "            current_token_count = sentence_token_count\n",
    "\n",
    "    # Add final chunk\n",
    "    if current_chunk:\n",
    "        chunks.append((current_chunk, current_token_count))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4e8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db24d7be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "102b8ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arq_files(input_path):\n",
    "    \"\"\"\n",
    "    Identify which JSON files to process based on the input path.\n",
    "    \"\"\"\n",
    "    files_to_process = []\n",
    "    if os.path.isdir(input_path):\n",
    "        # Process all JSON files in the directory\n",
    "        for filename in os.listdir(input_path):\n",
    "            if filename.endswith(\".json\"):\n",
    "                file_path = os.path.join(input_path, filename)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data_ = json.load(f)\n",
    "                    if len(data_) >= 1:\n",
    "                        files_to_process.append((data_, file_path))\n",
    "                    #     print(f\"Will process file: {file_path}\", len(data_))\n",
    "                    # else:\n",
    "                    #     print(\"Found empty file at\", file_path, len(data_))\n",
    "                # files_to_process.append(file_path)\n",
    "        print(\n",
    "            f\"Found {len(files_to_process)} JSON files to process in {input_path}\"\n",
    "        )\n",
    "    elif os.path.isfile(input_path) and input_path.endswith(\".json\"):\n",
    "        # Process a single JSON file\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data_ = json.load(f)\n",
    "            if len(data_) > 2:\n",
    "                files_to_process.append(data_, file_path)\n",
    "                print(f\"Will process single file: {input_path}\", len(data_))\n",
    "            else:\n",
    "                print(\"Found empty file at\", input_path)\n",
    "    else:\n",
    "        print(f\"Error: {input_path} is not a valid JSON file or directory\")\n",
    "\n",
    "    return files_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "038f86c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data_dir(data_dir, source_name, last_meta_id=-1):\n",
    "\n",
    "    if last_meta_id == -1:\n",
    "        last_meta_id = 0\n",
    "\n",
    "    files_to_process = get_arq_files(os.path.join(data_dir))\n",
    "    child_items = []\n",
    "    all_metadata = {}\n",
    "\n",
    "    for f_ in files_to_process:\n",
    "        \n",
    "        file_ = f_[0]\n",
    "        file_path = f_[1]\n",
    "\n",
    "        for tstamp in file_:\n",
    "            doc_ = file_[tstamp]\n",
    "            arq_meta, last_meta_id = get_arq_meta(doc_, last_meta_id)\n",
    "            arq_meta[\"filepath\"] = file_path\n",
    "            arq_meta[\"source_name\"] = source_name\n",
    "            \n",
    "            all_metadata[last_meta_id] = arq_meta\n",
    "            \n",
    "            for child_ in doc_['children']:\n",
    "                sent_ = preprocess_text(child_['text'])\n",
    "                child_chunks = chunk_text(sent_)\n",
    "\n",
    "                for chunk_ in child_chunks:\n",
    "                    chunk_item = {\n",
    "                        \"text\": chunk_[0], \n",
    "                        \"tokens\": chunk_[1],\n",
    "                        \"link\": child_['link'],\n",
    "                        \"tstamp\": tstamp,\n",
    "                        \"metadata_id\": last_meta_id,\n",
    "                        \"source\": source_name\n",
    "                        }\n",
    "                child_items.append(chunk_item)\n",
    "    return child_items, all_metadata, last_meta_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebde1a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_chunk(sentence):\n",
    "    sentence = sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    sentence = re.sub(cleanr, '', sentence)\n",
    "    sentence = sentence.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    sentence = unicodedata.normalize('NFKD', sentence)\n",
    "    sentence = ''.join(c for c in sentence if not unicodedata.combining(c))\n",
    "    sentence = re.sub(r'http\\S+', '', sentence)\n",
    "    sentence = re.sub(r'[^A-Za-z\\s]', '', sentence).lower().strip()\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec654116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    df_ = df.copy()\n",
    "    df_['normalized'] = df_['text'].apply(lambda x: normalize_chunk(x))\n",
    "    df_.sort_values(by=[\"normalized\", \"tstamp\"], inplace=True)\n",
    "    df_.drop_duplicates(subset=[\"normalized\"], keep=\"last\", inplace=True)\n",
    "    df_.drop(columns=['normalized'], inplace=True)\n",
    "    df_.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565764f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aafaf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05727fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b8ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8878c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72802007",
   "metadata": {},
   "source": [
    "## Retrieve Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bbbc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_map = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0995fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embeddings(docs, \n",
    "                   api_key=OPENAI_API_KEY, \n",
    "                   model=EMBEDDING_MODEL):\n",
    "    \"\"\"\n",
    "    Get embeddings for the provided texts using OpenAI API.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    responses = [] \n",
    "    embeddings = []\n",
    "\n",
    "    pbar = tqdm(docs)\n",
    "    for doc_i in pbar:\n",
    "        pbar.set_description(f\"Processing: get_embeddings\")\n",
    "        \n",
    "        resp_i = client.embeddings.create(input=doc_i, model=model)\n",
    "        responses.append(resp_i)\n",
    "\n",
    "        emb_i = np.array([item.embedding for item in resp_i.data])\n",
    "        embeddings.append(emb_i)\n",
    "\n",
    "\n",
    "    return np.concat(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def store_embeddings(docs, embeddings, metadatas, client, collection_name):\n",
    "\n",
    "    # Create or get collection\n",
    "    try:\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "        print(f\"Collection '{collection_name}' already exists. Adding documents...\")\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Creating new collection '{collection_name}'.\")\n",
    "        collection = client.create_collection(name=collection_name)\n",
    "\n",
    "\n",
    "    # Add documents to the collection\n",
    "    try:\n",
    "        collection.add(\n",
    "            documents=docs,\n",
    "            embeddings=embeddings,\n",
    "            ids=[f\"doc_{i}\" for i in range(len(docs))],\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "\n",
    "        print(f\"Added {len(docs)} documents to the collection.\")\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error adding documents to collection: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "        \n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1261f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def store_csv_embeddings(docs, embeddings, metadatas, dest):\n",
    "#     print(dest)\n",
    "#     print(len(docs))\n",
    "#     print(embeddings.shape)\n",
    "#     print(metadatas)\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7623882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"./../data/chroma_cravo\"\n",
    "\n",
    "client = chromadb.PersistentClient(path=db_path, \n",
    "                                   settings=Settings(anonymized_telemetry=False)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "463f65f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wiki_guerra_colonial_portuguesa',\n",
       " 'all',\n",
       " 'wiki_rev',\n",
       " 'wiki_estado_novo',\n",
       " 'publico',\n",
       " '50anos25abril',\n",
       " 'wiki_processo',\n",
       " 'wiki_constituicao',\n",
       " 'wiki_movimento_das_forcas_armadas',\n",
       " 'expresso',\n",
       " 'wiki_junta_de_salvacao_nacional']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dirs = os.listdir(data_dir)\n",
    "data_dirs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9a90009",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_categs = {\n",
    "    'wiki_guerra_colonial_portuguesa' : \"Wikipedia PT\",\n",
    "    'wiki_rev' : \"Wikipedia PT\",\n",
    "    'wiki_estado_novo' : \"Wikipedia PT\",\n",
    "    'publico' : \"Publico\",\n",
    "    '50anos25abril' : \"Web\",\n",
    "    'wiki_processo' : \"Wikipedia PT\",\n",
    "    'wiki_constituicao' : \"Wikipedia PT\",\n",
    "    'wiki_movimento_das_forcas_armadas' : \"Wikipedia PT\",\n",
    "    'expresso' : \"Expresso\",\n",
    "    'wiki_junta_de_salvacao_nacional' : \"Wikipedia PT\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b7d39fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 0 Wikipedia PT\n",
      "Found 17 JSON files to process in ./../data/res/wiki_guerra_colonial_portuguesa\n",
      "Starting: 75 Wikipedia PT\n",
      "Found 11 JSON files to process in ./../data/res/wiki_rev\n",
      "Starting: 186 Wikipedia PT\n",
      "Found 17 JSON files to process in ./../data/res/wiki_estado_novo\n",
      "Starting: 309 Publico\n",
      "Found 11 JSON files to process in ./../data/res/publico\n",
      "Starting: 907 Web\n",
      "Found 1 JSON files to process in ./../data/res/50anos25abril\n",
      "Starting: 951 Wikipedia PT\n",
      "Found 12 JSON files to process in ./../data/res/wiki_processo\n",
      "Starting: 982 Wikipedia PT\n",
      "Found 13 JSON files to process in ./../data/res/wiki_constituicao\n",
      "Starting: 1027 Wikipedia PT\n",
      "Found 17 JSON files to process in ./../data/res/wiki_movimento_das_forcas_armadas\n",
      "Starting: 1081 Expresso\n",
      "Found 9 JSON files to process in ./../data/res/expresso\n",
      "Starting: 1872 Wikipedia PT\n",
      "Found 13 JSON files to process in ./../data/res/wiki_junta_de_salvacao_nacional\n"
     ]
    }
   ],
   "source": [
    "last_meta_id = 0\n",
    "metadata_list = []\n",
    "df_list = []\n",
    "\n",
    "for source_dir, source_name in source_categs.items():\n",
    "    print(f\"Starting: {last_meta_id} {source_name}\")\n",
    "\n",
    "    source_data_dir = os.path.join(data_dir, source_dir)\n",
    "    source_data, source_meta, last_meta_id = process_data_dir(source_data_dir, source_name, last_meta_id)\n",
    "    source_df = pd.DataFrame(source_data)\n",
    "    source_df = remove_duplicates(source_df)\n",
    "    \n",
    "    metadata_list.append(source_meta)\n",
    "    df_list.append(source_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8023f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata_list[0]\n",
    "# df_list[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad98d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for source_i, source_name in enumerate(source_categs.items()):\n",
    "#     print(source_i, source_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7c3b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28d7b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_docs = 0\n",
    "# for df_ in df_list:\n",
    "#     print(df_.shape)\n",
    "#     count_docs += df_.shape[0]\n",
    "\n",
    "# print(count_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04881427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>link</th>\n",
       "      <th>tstamp</th>\n",
       "      <th>metadata_id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A primeira das celebrações realizou-se em 1963...</td>\n",
       "      <td>1733</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/200803161008...</td>\n",
       "      <td>20080316100800</td>\n",
       "      <td>65</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A primeira das celebrações realizou-se em 1963...</td>\n",
       "      <td>1756</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/200802150452...</td>\n",
       "      <td>20080215045248</td>\n",
       "      <td>66</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Após a independência, a grande maioria desses ...</td>\n",
       "      <td>3447</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/202406121933...</td>\n",
       "      <td>20240612193309</td>\n",
       "      <td>32</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cada redefinição do processo representava uma ...</td>\n",
       "      <td>4175</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201510051933...</td>\n",
       "      <td>20151005193344</td>\n",
       "      <td>20</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contudo, grupos organizados, num movimento con...</td>\n",
       "      <td>6347</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/202301220235...</td>\n",
       "      <td>20230122023529</td>\n",
       "      <td>6</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>A Junta de Salvação Nacional (JSN) foi um grup...</td>\n",
       "      <td>3026</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201510052300...</td>\n",
       "      <td>20151005230006</td>\n",
       "      <td>1949</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>A Junta de Salvação Nacional (JSN) foi um grup...</td>\n",
       "      <td>607</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201106151938...</td>\n",
       "      <td>20110615193844</td>\n",
       "      <td>2081</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>A Junta de Salvação Nacional (JSN) foi um grup...</td>\n",
       "      <td>615</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201107040751...</td>\n",
       "      <td>20110704075116</td>\n",
       "      <td>2080</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>A Junta de Salvação Nacional (JSN) foi um grup...</td>\n",
       "      <td>1835</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201311060820...</td>\n",
       "      <td>20131106082031</td>\n",
       "      <td>1944</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>Páginas para editores conectados saiba mais\\n\\...</td>\n",
       "      <td>3858</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/202202021613...</td>\n",
       "      <td>20220202161312</td>\n",
       "      <td>2078</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  tokens  \\\n",
       "0    A primeira das celebrações realizou-se em 1963...    1733   \n",
       "1    A primeira das celebrações realizou-se em 1963...    1756   \n",
       "2    Após a independência, a grande maioria desses ...    3447   \n",
       "3    Cada redefinição do processo representava uma ...    4175   \n",
       "4    Contudo, grupos organizados, num movimento con...    6347   \n",
       "..                                                 ...     ...   \n",
       "895  A Junta de Salvação Nacional (JSN) foi um grup...    3026   \n",
       "896  A Junta de Salvação Nacional (JSN) foi um grup...     607   \n",
       "897  A Junta de Salvação Nacional (JSN) foi um grup...     615   \n",
       "898  A Junta de Salvação Nacional (JSN) foi um grup...    1835   \n",
       "899  Páginas para editores conectados saiba mais\\n\\...    3858   \n",
       "\n",
       "                                                  link          tstamp  \\\n",
       "0    https://arquivo.pt/noFrame/replay/200803161008...  20080316100800   \n",
       "1    https://arquivo.pt/noFrame/replay/200802150452...  20080215045248   \n",
       "2    https://arquivo.pt/noFrame/replay/202406121933...  20240612193309   \n",
       "3    https://arquivo.pt/noFrame/replay/201510051933...  20151005193344   \n",
       "4    https://arquivo.pt/noFrame/replay/202301220235...  20230122023529   \n",
       "..                                                 ...             ...   \n",
       "895  https://arquivo.pt/noFrame/replay/201510052300...  20151005230006   \n",
       "896  https://arquivo.pt/noFrame/replay/201106151938...  20110615193844   \n",
       "897  https://arquivo.pt/noFrame/replay/201107040751...  20110704075116   \n",
       "898  https://arquivo.pt/noFrame/replay/201311060820...  20131106082031   \n",
       "899  https://arquivo.pt/noFrame/replay/202202021613...  20220202161312   \n",
       "\n",
       "     metadata_id        source  \n",
       "0             65  Wikipedia PT  \n",
       "1             66  Wikipedia PT  \n",
       "2             32  Wikipedia PT  \n",
       "3             20  Wikipedia PT  \n",
       "4              6  Wikipedia PT  \n",
       "..           ...           ...  \n",
       "895         1949  Wikipedia PT  \n",
       "896         2081  Wikipedia PT  \n",
       "897         2080  Wikipedia PT  \n",
       "898         1944  Wikipedia PT  \n",
       "899         2078  Wikipedia PT  \n",
       "\n",
       "[900 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat = pd.concat(df_list, ignore_index=True)\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04073732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embedding(doc, \n",
    "                   api_key=OPENAI_API_KEY, \n",
    "                   model=EMBEDDING_MODEL):\n",
    "    \"\"\"\n",
    "    Get embeddings for the provided texts using OpenAI API.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    response = client.embeddings.create(input=doc, model=model)\n",
    "\n",
    "    embedding = np.array([item.embedding for item in response.data])\n",
    "\n",
    "\n",
    "    return embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7aaa3d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = \"./../data/embeddings/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4e5fa70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1313d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_tocsv(embeddings_path, df_concat):\n",
    "    with tqdm(total=df_concat.shape[0]) as df_bar:\n",
    "\n",
    "        for i, row_i in df_concat.iterrows():\n",
    "            df_bar.set_description(f\"Processing embeddings\")\n",
    "\n",
    "            em_i = get_embedding(row_i['text'])\n",
    "            path_i = os.path.join(embeddings_path,f\"{i}.csv\")\n",
    "            pd.DataFrame(em_i).to_csv(path_i, index=False, header=False)\n",
    "\n",
    "            df_bar.update(1)\n",
    "    df_bar.close()\n",
    "\n",
    "# embeddings_tocsv(embeddings_path, df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca706aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_concat[['text']].to_csv(os.path.join(embeddings_path, \"documents.csv\"), index_label=\"id\", )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa46b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3be37934",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.read_csv(os.path.join(embeddings_path, \"documents.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ff76c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata(embeddings_path, metadata_list):\n",
    "    meta_concat = {}\n",
    "\n",
    "    for i in metadata_list:\n",
    "        meta_concat = meta_concat | i\n",
    "\n",
    "    with open(os.path.join(embeddings_path,\"metadata.json\"), \"w\") as outfile:\n",
    "        outfile.write(json.dumps(meta_concat, indent=2))\n",
    "\n",
    "def load_metadata(meta_path):\n",
    "    with open(meta_path, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0cc4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_concat = load_metadata(os.path.join(embeddings_path,\"metadata.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f13791a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(docs, embeddings, metadatas, client, collection_name):\n",
    "\n",
    "    # Create or get collection\n",
    "    try:\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "        print(f\"Collection '{collection_name}' already exists. Adding documents...\")\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Creating new collection '{collection_name}'.\")\n",
    "        collection = client.create_collection(name=collection_name)\n",
    "\n",
    "\n",
    "    # Add documents to the collection\n",
    "    try:\n",
    "        collection.add(\n",
    "            documents=docs,\n",
    "            embeddings=embeddings,\n",
    "            ids=[f\"doc_{i}\" for i in range(len(docs))],\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "\n",
    "        print(f\"Added {len(docs)} documents to the collection.\")\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error adding documents to collection: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "        \n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c5a111c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>link</th>\n",
       "      <th>tstamp</th>\n",
       "      <th>metadata_id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A primeira das celebrações realizou-se em 1963...</td>\n",
       "      <td>1733</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/200803161008...</td>\n",
       "      <td>20080316100800</td>\n",
       "      <td>65</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A primeira das celebrações realizou-se em 1963...</td>\n",
       "      <td>1756</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/200802150452...</td>\n",
       "      <td>20080215045248</td>\n",
       "      <td>66</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Após a independência, a grande maioria desses ...</td>\n",
       "      <td>3447</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/202406121933...</td>\n",
       "      <td>20240612193309</td>\n",
       "      <td>32</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cada redefinição do processo representava uma ...</td>\n",
       "      <td>4175</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201510051933...</td>\n",
       "      <td>20151005193344</td>\n",
       "      <td>20</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contudo, grupos organizados, num movimento con...</td>\n",
       "      <td>6347</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/202301220235...</td>\n",
       "      <td>20230122023529</td>\n",
       "      <td>6</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>A Junta de Salvação Nacional (JSN) foi um grup...</td>\n",
       "      <td>3026</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201510052300...</td>\n",
       "      <td>20151005230006</td>\n",
       "      <td>1949</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>A Junta de Salvação Nacional (JSN) foi um grup...</td>\n",
       "      <td>607</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201106151938...</td>\n",
       "      <td>20110615193844</td>\n",
       "      <td>2081</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>A Junta de Salvação Nacional (JSN) foi um grup...</td>\n",
       "      <td>615</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201107040751...</td>\n",
       "      <td>20110704075116</td>\n",
       "      <td>2080</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>A Junta de Salvação Nacional (JSN) foi um grup...</td>\n",
       "      <td>1835</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201311060820...</td>\n",
       "      <td>20131106082031</td>\n",
       "      <td>1944</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>Páginas para editores conectados saiba mais\\n\\...</td>\n",
       "      <td>3858</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/202202021613...</td>\n",
       "      <td>20220202161312</td>\n",
       "      <td>2078</td>\n",
       "      <td>Wikipedia PT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  tokens  \\\n",
       "0    A primeira das celebrações realizou-se em 1963...    1733   \n",
       "1    A primeira das celebrações realizou-se em 1963...    1756   \n",
       "2    Após a independência, a grande maioria desses ...    3447   \n",
       "3    Cada redefinição do processo representava uma ...    4175   \n",
       "4    Contudo, grupos organizados, num movimento con...    6347   \n",
       "..                                                 ...     ...   \n",
       "895  A Junta de Salvação Nacional (JSN) foi um grup...    3026   \n",
       "896  A Junta de Salvação Nacional (JSN) foi um grup...     607   \n",
       "897  A Junta de Salvação Nacional (JSN) foi um grup...     615   \n",
       "898  A Junta de Salvação Nacional (JSN) foi um grup...    1835   \n",
       "899  Páginas para editores conectados saiba mais\\n\\...    3858   \n",
       "\n",
       "                                                  link          tstamp  \\\n",
       "0    https://arquivo.pt/noFrame/replay/200803161008...  20080316100800   \n",
       "1    https://arquivo.pt/noFrame/replay/200802150452...  20080215045248   \n",
       "2    https://arquivo.pt/noFrame/replay/202406121933...  20240612193309   \n",
       "3    https://arquivo.pt/noFrame/replay/201510051933...  20151005193344   \n",
       "4    https://arquivo.pt/noFrame/replay/202301220235...  20230122023529   \n",
       "..                                                 ...             ...   \n",
       "895  https://arquivo.pt/noFrame/replay/201510052300...  20151005230006   \n",
       "896  https://arquivo.pt/noFrame/replay/201106151938...  20110615193844   \n",
       "897  https://arquivo.pt/noFrame/replay/201107040751...  20110704075116   \n",
       "898  https://arquivo.pt/noFrame/replay/201311060820...  20131106082031   \n",
       "899  https://arquivo.pt/noFrame/replay/202202021613...  20220202161312   \n",
       "\n",
       "     metadata_id        source  \n",
       "0             65  Wikipedia PT  \n",
       "1             66  Wikipedia PT  \n",
       "2             32  Wikipedia PT  \n",
       "3             20  Wikipedia PT  \n",
       "4              6  Wikipedia PT  \n",
       "..           ...           ...  \n",
       "895         1949  Wikipedia PT  \n",
       "896         2081  Wikipedia PT  \n",
       "897         2080  Wikipedia PT  \n",
       "898         1944  Wikipedia PT  \n",
       "899         2078  Wikipedia PT  \n",
       "\n",
       "[900 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d591d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precomputed_embeddings(embeddings_path, df_concat):\n",
    "    df_embeddings = None\n",
    "    with tqdm(total=df_concat.shape[0]) as df_bar:\n",
    "\n",
    "        for i, row_i in df_concat.iterrows():\n",
    "            df_bar.set_description(f\"Processing embeddings\")\n",
    "            path_i = os.path.join(embeddings_path,f\"{i}.csv\")\n",
    "            em_i = pd.read_csv(path_i, header=None)\n",
    "\n",
    "            if i == 0:\n",
    "                df_embeddings = em_i.copy()\n",
    "            else: \n",
    "                df_embeddings = pd.concat([df_embeddings, em_i], axis=0, ignore_index=True)\n",
    "\n",
    "            df_bar.update(1)\n",
    "    df_bar.close()\n",
    "    return df_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2e9c404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34d62cb8cbc4ad6b4784b09626257bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(900, 1536)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_path = \"./../data/embeddings/\"\n",
    "\n",
    "df_embeddings = get_precomputed_embeddings(embeddings_path, df_concat)\n",
    "\n",
    "df_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f967fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8c1b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"./../data/chroma_cravo\"\n",
    "\n",
    "client = chromadb.PersistentClient(path=db_path, \n",
    "                                   settings=Settings(anonymized_telemetry=False)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdf6feed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new collection 'cravo'.\n",
      "Added 900 documents to the collection.\n"
     ]
    }
   ],
   "source": [
    "docs_ = df_concat['text'].values\n",
    "embeddings = df_embeddings.values\n",
    "metadatas = [{\"link\": m[0], \"m_id\": m[1]} for m in df_concat[['link', 'metadata_id']].values]\n",
    "docs = docs_.tolist()\n",
    "\n",
    "store_embeddings(docs, embeddings, metadatas, client, \"cravo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981aa642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed73cebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab239536",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ed9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
